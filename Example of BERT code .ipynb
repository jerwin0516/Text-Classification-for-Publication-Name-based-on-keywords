{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-16T04:34:24.029443Z","iopub.execute_input":"2023-06-16T04:34:24.030999Z","iopub.status.idle":"2023-06-16T04:34:24.052779Z","shell.execute_reply.started":"2023-06-16T04:34:24.030945Z","shell.execute_reply":"2023-06-16T04:34:24.051311Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/input/biblio-management/Bibliometrix data.csv\n/kaggle/input/cleaned-train/processed_train.csv\n/kaggle/input/cleaned-test/processed_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_csv (\"/kaggle/input/cleaned-train/processed_train.csv\")\ntest_data = pd.read_csv (\"/kaggle/input/cleaned-test/processed_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-16T04:34:24.054585Z","iopub.execute_input":"2023-06-16T04:34:24.055807Z","iopub.status.idle":"2023-06-16T04:34:24.079122Z","shell.execute_reply.started":"2023-06-16T04:34:24.055756Z","shell.execute_reply":"2023-06-16T04:34:24.077552Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data.info()\ntest_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-16T04:34:24.081071Z","iopub.execute_input":"2023-06-16T04:34:24.081483Z","iopub.status.idle":"2023-06-16T04:34:24.103280Z","shell.execute_reply.started":"2023-06-16T04:34:24.081450Z","shell.execute_reply":"2023-06-16T04:34:24.102113Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 520 entries, 0 to 519\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   ID      520 non-null    object\n 1   SO      520 non-null    object\ndtypes: object(2)\nmemory usage: 8.2+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 130 entries, 0 to 129\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   ID      130 non-null    object\n 1   SO      130 non-null    object\ndtypes: object(2)\nmemory usage: 2.2+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom tqdm.notebook import tqdm\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Set seeds for reproducibility\nseed_val = 808\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntorch.backends.cudnn.deterministic = True\n\n# Assume training_set DataFrame is loaded and ready\nX_train_full = train_data['ID']\ny_train_full = train_data['SO']\n\n# Split the training_set into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=808)\n\n# Assume test_set DataFrame is loaded and ready\nX_test = test_data['ID']\ny_test = test_data['SO']\n\n# Encode Labels\npossible_labels = y_train_full.unique()\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\n\ny_train = y_train.replace(label_dict)\ny_val = y_val.replace(label_dict)\ny_test = y_test.replace(label_dict)\n\n# Tokenization\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n# Encoding Data\nencoded_data_train = tokenizer.batch_encode_plus(X_train.values, add_special_tokens=True, return_attention_mask=True, \n                                                 padding=True, max_length=60, return_tensors='pt')\nencoded_data_val = tokenizer.batch_encode_plus(X_val.values, add_special_tokens=True, return_attention_mask=True, \n                                               padding=True, max_length=60, return_tensors='pt')\nencoded_data_test = tokenizer.batch_encode_plus(X_test.values, add_special_tokens=True, return_attention_mask=True, \n                                                padding=True, max_length=60, return_tensors='pt')\n# Separate Input IDs, Attention Masks, Labels \ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(y_train.values)\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(y_val.values)\ninput_ids_test = encoded_data_test['input_ids']\nattention_masks_test = encoded_data_test['attention_mask']\nlabels_test = torch.tensor(y_test.values)\n\n# Create Tensor Dataset\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\ndataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n\n# Define the batch size\nbatch_size = 16\n\n# Create DataLoaders\ndataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\ndataloader_val = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\ndataloader_test = DataLoader(dataset_test, sampler=SequentialSampler(dataset_test), batch_size=batch_size)\n\n# Load the Pretrained BERT model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict), output_attentions=False, \n                                                      output_hidden_states=False)\n\n# Create the optimizer\noptimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n\n# Number of training epochs \nepochs = 40\n\n# Total number of training steps\ntotal_steps = len(dataloader_train) * epochs\n\n# Set up the learning rate scheduler\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Defining the function to evaluate the model\ndef evaluate(dataloader):\n    model.eval()\n    loss_val_total = 0\n    predictions, true_vals = [], []\n    for batch in dataloader:\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],}\n        with torch.no_grad():        \n            outputs = model(**inputs)\n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n\n    loss_val_avg = loss_val_total/len(dataloader)\n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n    return loss_val_avg, predictions, true_vals\n\nbest_val_loss = float('inf')\nbest_val_f1 = -float('inf')\nbest_val_accuracy = -float('inf')\nepochs_no_improve = 0\nearly_stopping_patience = 5  \n\n# Training loop with early stopping\nfor epoch in tqdm(range(1, epochs+1)):\n    model.train()\n    loss_train_total = 0\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n\n    for batch in progress_bar:\n        model.zero_grad()\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],}       \n\n        outputs = model(**inputs)\n        loss = outputs[0]\n        torch.cuda.empty_cache()\n        loss_train_total += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    loss_train_avg = loss_train_total/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n\n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    val_f1 = f1_score(true_vals, np.argmax(predictions, axis=1), average='macro')\n    val_accuracy = accuracy_score(true_vals, np.argmax(predictions, axis=1))\n\n    tqdm.write(f'Validation Loss: {val_loss}')\n    tqdm.write(f'F1 Score (macro): {val_f1}')\n    tqdm.write(f'Validation Accuracy: {val_accuracy}')\n    improvement = False\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_valid_loss_weights = model.state_dict().copy()\n        torch.save(model.state_dict(), 'best_val_loss_model.pt')\n        improvement = True\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_f1_weights = model.state_dict().copy()\n        torch.save(model.state_dict(), 'best_val_f1_model.pt')\n        improvement = True\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        best_accuracy_weights = model.state_dict().copy()\n        torch.save(model.state_dict(), 'best_val_accuracy_model.pt')\n        improvement = True\n    if improvement:\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n    if epochs_no_improve >= early_stopping_patience:\n        print(\"Early stopping triggered.\")\n        break\n        \n# Evaluation stage on test data for the best model weights based on validation loss, F1 Score, and accuracy\nmodel.load_state_dict(best_valid_loss_weights)\nval_loss, predictions_val, true_vals_val = evaluate(dataloader_test)\ntest_f1_val = f1_score(predictions_val.argmax(axis=1), true_vals_val, average='macro')\ntest_accuracy_val = accuracy_score(predictions_val.argmax(axis=1), true_vals_val)\nprint(\"Model with best validation loss:\")\nprint(\"Test F1 Score (macro):\", test_f1_val)\n\nmodel.load_state_dict(best_f1_weights)\nval_loss, predictions_f1, true_vals_f1 = evaluate(dataloader_test)\ntest_f1_f1 = f1_score(predictions_f1.argmax(axis=1), true_vals_f1, average='macro')\ntest_accuracy_f1 = accuracy_score(predictions_f1.argmax(axis=1), true_vals_f1)\nprint(\"Model with best F1 Score:\")\nprint(\"Test F1 Score (macro):\", test_f1_f1)\n\nmodel.load_state_dict(best_accuracy_weights)\nval_loss, predictions_accuracy, true_vals_accuracy = evaluate(dataloader_test)\ntest_f1_accuracy = f1_score(predictions_accuracy.argmax(axis=1), true_vals_accuracy, average='macro')\ntest_accuracy_accuracy = accuracy_score(predictions_accuracy.argmax(axis=1), true_vals_accuracy)\nprint(\"Model with best accuracy:\")\nprint(\"Test F1 Score (macro):\", test_f1_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T04:34:24.104975Z","iopub.execute_input":"2023-06-16T04:34:24.105355Z","iopub.status.idle":"2023-06-16T05:06:49.063469Z","shell.execute_reply.started":"2023-06-16T04:34:24.105301Z","shell.execute_reply":"2023-06-16T05:06:49.061888Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd2e5c1bc2646d08ce2cd79980aa82d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 1\nTraining loss: 4.7917390419886665\nValidation Loss: 4.630096026829311\nF1 Score (macro): 0.006371816649952586\nValidation Accuracy: 0.14423076923076922\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2\nTraining loss: 4.5623228549957275\nValidation Loss: 4.474183286939349\nF1 Score (macro): 0.007495694733979878\nValidation Accuracy: 0.15384615384615385\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3\nTraining loss: 4.413364758858314\nValidation Loss: 4.392590182168143\nF1 Score (macro): 0.006637806637806638\nValidation Accuracy: 0.125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 4\nTraining loss: 4.305730984761165\nValidation Loss: 4.3373101098196845\nF1 Score (macro): 0.006953498478922207\nValidation Accuracy: 0.14423076923076922\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 5\nTraining loss: 4.219302672606248\nValidation Loss: 4.301808016640799\nF1 Score (macro): 0.008330652398449009\nValidation Accuracy: 0.16346153846153846\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 6\nTraining loss: 4.141314607400161\nValidation Loss: 4.293782642909458\nF1 Score (macro): 0.008793803380193465\nValidation Accuracy: 0.16346153846153846\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 7\nTraining loss: 4.05183219909668\nValidation Loss: 4.250149829047067\nF1 Score (macro): 0.00871578811951243\nValidation Accuracy: 0.16346153846153846\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 8\nTraining loss: 4.01509690284729\nValidation Loss: 4.234695775168283\nF1 Score (macro): 0.009229616922575113\nValidation Accuracy: 0.17307692307692307\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 9\nTraining loss: 3.962422114152175\nValidation Loss: 4.197130611964634\nF1 Score (macro): 0.009514026463179006\nValidation Accuracy: 0.17307692307692307\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 10\nTraining loss: 3.8819489662463846\nValidation Loss: 4.202595744814191\nF1 Score (macro): 0.009000415780076796\nValidation Accuracy: 0.16346153846153846\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 11\nTraining loss: 3.823839517740103\nValidation Loss: 4.189733266830444\nF1 Score (macro): 0.00965364775239499\nValidation Accuracy: 0.17307692307692307\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 12\nTraining loss: 3.771931153077346\nValidation Loss: 4.191723108291626\nF1 Score (macro): 0.015314519633819797\nValidation Accuracy: 0.18269230769230768\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 13\nTraining loss: 3.7096719650121837\nValidation Loss: 4.187514577593122\nF1 Score (macro): 0.016417414423396476\nValidation Accuracy: 0.21153846153846154\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 14\nTraining loss: 3.6849987598565908\nValidation Loss: 4.185874700546265\nF1 Score (macro): 0.013275613275613277\nValidation Accuracy: 0.18269230769230768\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 15\nTraining loss: 3.6178146050526547\nValidation Loss: 4.17342403956822\nF1 Score (macro): 0.013681776106366741\nValidation Accuracy: 0.18269230769230768\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 16\nTraining loss: 3.549061500109159\nValidation Loss: 4.178245340074811\nF1 Score (macro): 0.016340069353853785\nValidation Accuracy: 0.20192307692307693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 17\nTraining loss: 3.5227262515288134\nValidation Loss: 4.189133984701974\nF1 Score (macro): 0.016310579172567292\nValidation Accuracy: 0.19230769230769232\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 18\nTraining loss: 3.473017903474661\nValidation Loss: 4.168622936521258\nF1 Score (macro): 0.015002876941315359\nValidation Accuracy: 0.20192307692307693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 19\nTraining loss: 3.4220586006457987\nValidation Loss: 4.1614640440259665\nF1 Score (macro): 0.014527845036319613\nValidation Accuracy: 0.20192307692307693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 20\nTraining loss: 3.411489560053899\nValidation Loss: 4.186472858701434\nF1 Score (macro): 0.014145713320935203\nValidation Accuracy: 0.18269230769230768\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 21\nTraining loss: 3.35160454419943\nValidation Loss: 4.191263096673148\nF1 Score (macro): 0.01493182249882401\nValidation Accuracy: 0.19230769230769232\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 22\nTraining loss: 3.318536254075857\nValidation Loss: 4.176703350884574\nF1 Score (macro): 0.01596045197740113\nValidation Accuracy: 0.21153846153846154\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 23\nTraining loss: 3.286200312467722\nValidation Loss: 4.179150138582502\nF1 Score (macro): 0.015266081871345028\nValidation Accuracy: 0.20192307692307693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 24\nTraining loss: 3.2458109122056227\nValidation Loss: 4.1627539566584995\nF1 Score (macro): 0.015673400673400676\nValidation Accuracy: 0.21153846153846154\nEarly stopping triggered.\nModel with best validation loss:\nTest Accuracy: 0.2\nTest F1 Score (macro): 0.011822876346854763\nModel with best F1 Score:\nTest Accuracy: 0.2\nTest F1 Score (macro): 0.011822876346854763\nModel with best accuracy:\nTest Accuracy: 0.2\nTest F1 Score (macro): 0.011822876346854763\n","output_type":"stream"}]}]}